#!/usr/bin/env python3
# SPDX-License-Identifier: Apache-2.0
# SPDX-FileCopyrightText: 2026 Sigyn AI Contributors
"""
Export trained YOLO models to device-specific formats.

This script handles the complete export pipeline:
1. Load trained .pt model
2. Export to ONNX with device-specific settings
3. Optionally compile to target format (.hef, .blob, .engine)

Usage:
    python export.py --model models/checkpoints/can_detector_pihat_v1/weights/best.pt --device pi5_hailo8
    python export.py --model models/checkpoints/can_detector_pihat_v1/weights/best.pt --device oakd_lite
    python export.py --model models/checkpoints/can_detector_pihat_v1/weights/best.pt --device jetson_orin_nano
"""

import argparse
import sys
import yaml
import shutil
from pathlib import Path
from ultralytics import YOLO
import onnx
from onnxsim import simplify


def load_device_config(device_id):
    """Load device configuration."""
    config_path = Path(f"configs/devices/{device_id}.yaml")
    if not config_path.exists():
        raise FileNotFoundError(f"Device config not found: {config_path}")
    
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    
    return config


def export_to_onnx(model_path, device_config, output_dir, imgsz=None):
    """Export PyTorch model to ONNX format."""
    model_path = Path(model_path)
    if not model_path.exists():
        raise FileNotFoundError(f"Model not found: {model_path}")
    
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Load model
    model = YOLO(str(model_path))
    
    # Get export settings
    export_settings = device_config['export']
    onnx_settings = export_settings.get('onnx_export', {})
    
    # Determine input size
    if imgsz is None:
        # Use device config default
        supported_sizes = device_config['specifications']['supported_input_sizes']
        imgsz = 640 if 640 in supported_sizes else supported_sizes[-1]
    
    print(f"\nüì¶ Exporting to ONNX...")
    print(f"   Model: {model_path.name}")
    print(f"   Input size: {imgsz}x{imgsz}")
    print(f"   Output: {output_dir}")
    
    # Export to ONNX
    onnx_path = model.export(
        format='onnx',
        imgsz=imgsz,
        opset=export_settings.get('opset', 11),
        simplify=onnx_settings.get('simplify', True),
        dynamic=onnx_settings.get('dynamic', False),
        nms=onnx_settings.get('nms', False),
    )
    
    # Move ONNX to output directory
    onnx_path = Path(onnx_path)
    output_onnx = output_dir / f"{model_path.stem}.onnx"
    shutil.copy(onnx_path, output_onnx)
    
    print(f"‚úÖ ONNX export complete: {output_onnx}")
    
    # Simplify ONNX if requested (extra simplification pass)
    if onnx_settings.get('simplify', False):
        print("üîß Running additional ONNX simplification...")
        try:
            model_onnx = onnx.load(str(output_onnx))
            model_simp, check = simplify(model_onnx)
            if check:
                onnx.save(model_simp, str(output_onnx))
                print("‚úÖ ONNX simplified successfully")
            else:
                print("‚ö†Ô∏è  ONNX simplification validation failed, keeping original")
        except Exception as e:
            print(f"‚ö†Ô∏è  ONNX simplification failed: {e}")
            print("   Keeping original ONNX")
    
    return output_onnx


def compile_hailo(onnx_path, device_config, output_dir):
    """Compile ONNX to Hailo .hef format using Docker."""
    print(f"\nüî® Compiling for Hailo-8...")
    print(f"   This requires the Hailo Docker container")
    print(f"   See: docs/HAILO_COMPILATION.md")
    
    hailo_settings = device_config['export'].get('hailo_compilation', {})
    
    # Generate compilation script
    compile_script = f"""#!/bin/bash
# Hailo compilation script
# Generated by sigyn_ai export.py

# This script should be run inside the Hailo Docker container:
# docker run -it --rm -v /path/to/sigyn_ai:/workspace hailo_suite bash

hailo compile \\
    --model {onnx_path.name} \\
    --hw-arch hailo8 \\
    --optimization-level {hailo_settings.get('optimization_level', 2)} \\
    --batch-size {hailo_settings.get('batch_size', 1)} \\
    {'--compression' if hailo_settings.get('compression', True) else ''} \\
    --output {onnx_path.stem}.hef

# Expected output: {onnx_path.stem}.hef
"""
    
    script_path = output_dir / "compile_hailo.sh"
    with open(script_path, 'w') as f:
        f.write(compile_script)
    script_path.chmod(0o755)
    
    print(f"\nüìù Compilation script saved: {script_path}")
    print(f"\nüìã Manual steps required:")
    print(f"   1. Start Hailo Docker container:")
    print(f"      docker run -it --rm -v {Path.cwd()}:/workspace hailo8_ai_sw_suite_2025-10 bash")
    print(f"   2. Inside container, navigate to:")
    print(f"      cd /workspace/{output_dir}")
    print(f"   3. Run compilation script:")
    print(f"      ./compile_hailo.sh")
    print(f"   4. Exit container when done")
    print(f"\n   OR: Use the automated Docker wrapper:")
    print(f"      python docker/run_hailo_compile.py --onnx {onnx_path}")


def compile_oakd(onnx_path, device_config, output_dir, imgsz=416):
    """Compile ONNX to OAK-D .blob format using modelconverter."""
    print(f"\nüî® Compiling for OAK-D using ModelConverter...")
    
    blob_settings = device_config['export'].get('blob_compilation', {})
    
    # Create modelconverter config
    model_name = onnx_path.stem
    workspace_root = Path.cwd()
    shared_dir = workspace_root / "shared_with_container"
    
    # Set up directory structure
    (shared_dir / "models").mkdir(parents=True, exist_ok=True)
    (shared_dir / "configs").mkdir(parents=True, exist_ok=True)
    (shared_dir / "outputs").mkdir(parents=True, exist_ok=True)
    
    # Copy ONNX to shared directory
    shared_onnx = shared_dir / "models" / onnx_path.name
    shutil.copy(onnx_path, shared_onnx)
    
    # Create modelconverter config
    config_content = f"""name: {model_name}

# Input model path (relative to shared_with_container directory)
input_model: models/{onnx_path.name}

# Input configuration
inputs:
  - name: images
    shape: [1, 3, {imgsz}, {imgsz}]
    layout: NCHW
    data_type: float32
    # YOLOv5 preprocessing: images / 255.0
    scale_values: [255.0, 255.0, 255.0]
    mean_values: [0.0, 0.0, 0.0]
    encoding:
      from: RGB
      to: RGB  # YOLOv5 is trained on RGB

# Output configuration
outputs:
  - name: output0  # YOLOv5 output name

# RVC2 (OAK-D) specific settings
rvc2:
  number_of_shaves: {blob_settings.get('shaves', 6)}  # OAK-D Lite has 6 shaves
  compress_to_fp16: true
  superblob: false  # Regular blob for compatibility
"""
    
    config_path = shared_dir / "configs" / f"{model_name}_modelconv.yaml"
    with open(config_path, 'w') as f:
        f.write(config_content)
    
    print(f"üìã Created ModelConverter config: {config_path}")
    
    try:
        import subprocess
        
        print("üî® Running ModelConverter (this may take 1-3 minutes)...")
        
        # Run modelconverter via subprocess
        result = subprocess.run(
            [
                "modelconverter", "convert", "rvc2",
                "--tool-version", "2022.3.0",
                "--path", f"shared_with_container/configs/{model_name}_modelconv.yaml"
            ],
            cwd=str(workspace_root),
            capture_output=False,  # Don't capture, let it print directly
            text=True
        )
        
        if result.returncode != 0:
            print(f"‚ùå ModelConverter failed with return code: {result.returncode}")
            return None
        
        # Find the most recent output directory
        import glob
        output_dirs = sorted(
            (shared_dir / "outputs").glob(f"{model_name}_to_rvc2_*"),
            key=lambda p: p.stat().st_mtime,
            reverse=True
        )
        
        if not output_dirs:
            print("‚ö†Ô∏è  Could not locate output directory")
            return None
        
        latest_output = output_dirs[0]
        blob_path = latest_output / "best.blob"
        
        if not blob_path.exists():
            print(f"‚ö†Ô∏è  Could not locate output blob in {latest_output}")
            return None
        
        # Copy blob to output directory
        final_blob = output_dir / f"{model_name}.blob"
        shutil.copy(blob_path, final_blob)
        
        print(f"‚úÖ OAK-D .blob compiled: {final_blob}")
        return final_blob
            
    except ImportError:
        print("‚ö†Ô∏è  modelconverter not installed")
        print("\nüìù Install with: pip install modelconv")
        return None
    except Exception as e:
        print(f"‚ùå Error running ModelConverter: {e}")
        return None


def compile_jetson(onnx_path, device_config, output_dir):
    """Instructions for compiling ONNX to TensorRT .engine on Jetson."""
    print(f"\nüî® Preparing for Jetson TensorRT compilation...")
    print(f"   ‚ö†Ô∏è  TensorRT engines MUST be compiled on the target device")
    print(f"   (engines are not portable across systems)")
    
    tensorrt_settings = device_config['export'].get('tensorrt_compilation', {})
    
    # Generate compilation script for Jetson
    compile_script = f"""#!/usr/bin/env python3
# TensorRT compilation script for Jetson
# Run this script ON THE JETSON device

import tensorrt as trt

# Create logger and builder
TRT_LOGGER = trt.Logger(trt.Logger.WARNING)
builder = trt.Builder(TRT_LOGGER)
network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
parser = trt.OnnxParser(network, TRT_LOGGER)

# Parse ONNX model
with open('{onnx_path.name}', 'rb') as model:
    if not parser.parse(model.read()):
        for error in range(parser.num_errors):
            print(parser.get_error(error))
        raise RuntimeError("Failed to parse ONNX model")

# Create builder config
config = builder.create_builder_config()
config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, {tensorrt_settings.get('workspace_mb', 4096)} * 1024 * 1024)

# Set precision
if '{tensorrt_settings.get('precision', 'fp16')}' == 'fp16':
    config.set_flag(trt.BuilderFlag.FP16)
elif '{tensorrt_settings.get('precision', 'fp16')}' == 'int8':
    config.set_flag(trt.BuilderFlag.INT8)

# Build engine
print("Building TensorRT engine (this may take 5-20 minutes)...")
serialized_engine = builder.build_serialized_network(network, config)

# Save engine
with open('{onnx_path.stem}.engine', 'wb') as f:
    f.write(serialized_engine)

print("TensorRT engine saved: {onnx_path.stem}.engine")
"""
    
    script_path = output_dir / "compile_tensorrt.py"
    with open(script_path, 'w') as f:
        f.write(compile_script)
    script_path.chmod(0o755)
    
    print(f"\nüìù Compilation script saved: {script_path}")
    print(f"\nüìã Deploy to Jetson and compile:")
    print(f"   1. Copy ONNX and script to Jetson:")
    print(f"      scp {onnx_path} {script_path} {device_config['deployment']['host']}:~/")
    print(f"   2. SSH to Jetson and run:")
    print(f"      python3 compile_tensorrt.py")
    print(f"   3. Copy .engine back if needed")
    print(f"\n   OR: Use the deployment tool:")
    print(f"      python src/deployment/deploy.py --model {onnx_path.stem} --target jetson_orin_nano --compile")


def extract_labels(model_path, output_dir):
    """Extract class labels from model and save to labels.txt."""
    model = YOLO(str(model_path))
    names = model.names
    
    labels_path = output_dir / "labels.txt"
    with open(labels_path, 'w') as f:
        for idx in sorted(names.keys()):
            f.write(f"{names[idx]}\n")
    
    print(f"üìù Labels saved: {labels_path}")
    return labels_path


def main():
    parser = argparse.ArgumentParser(
        description='Export YOLO models for Sigyn AI devices',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Export for Pi 5 + Hailo-8
  python export.py --model models/checkpoints/can_detector_pihat_v1/weights/best.pt --device pi5_hailo8
  
  # Export for OAK-D
  python export.py --model models/checkpoints/can_detector_pihat_v1/weights/best.pt --device oakd_lite
  
  # Export for Jetson
  python export.py --model models/checkpoints/can_detector_pihat_v1/weights/best.pt --device jetson_orin_nano
        """
    )
    
    parser.add_argument(
        '--model',
        type=str,
        required=True,
        help='Path to trained .pt model'
    )
    
    parser.add_argument(
        '--device',
        type=str,
        required=True,
        choices=['pi5_hailo8', 'oakd_lite', 'jetson_orin_nano'],
        help='Target device identifier'
    )
    
    parser.add_argument(
        '--output',
        type=str,
        default=None,
        help='Output directory (default: models/exported/<model_name>/<device>)'
    )
    
    parser.add_argument(
        '--compile',
        action='store_true',
        help='Attempt automatic compilation to device format (may require additional tools)'
    )
    
    parser.add_argument(
        '--imgsz',
        type=int,
        default=None,
        help='Input image size (must match training size). If not specified, uses device config default.'
    )
    
    args = parser.parse_args()
    
    # Load device configuration
    print(f"üîç Loading device config: {args.device}")
    device_config = load_device_config(args.device)
    print(f"   Device: {device_config['device']['name']}")
    print(f"   Accelerator: {device_config['device']['accelerator']}")
    
    # Determine output directory
    if args.output:
        output_dir = Path(args.output)
    else:
        model_name = Path(args.model).stem
        output_dir = Path(f"models/exported/{model_name}/{args.device}")
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    try:
        # Export to ONNX
        onnx_path = export_to_onnx(args.model, device_config, output_dir, imgsz=args.imgsz)
        
        # Extract labels
        labels_path = extract_labels(args.model, output_dir)
        
        # Compile to device-specific format if requested
        if args.compile:
            # Determine the imgsz used for export
            export_imgsz = args.imgsz
            if export_imgsz is None:
                supported_sizes = device_config['specifications']['supported_input_sizes']
                export_imgsz = 640 if 640 in supported_sizes else supported_sizes[-1]
            
            if args.device == 'pi5_hailo8':
                compile_hailo(onnx_path, device_config, output_dir)
            elif args.device == 'oakd_lite':
                compile_oakd(onnx_path, device_config, output_dir, imgsz=export_imgsz)
            elif args.device == 'jetson_orin_nano':
                compile_jetson(onnx_path, device_config, output_dir)
        else:
            print(f"\nüí° Tip: Add --compile flag to automatically compile to {args.device} format")
            print(f"   (may require additional tools or Docker)")
        
        print(f"\n‚úÖ Export complete!")
        print(f"   Output directory: {output_dir}")
        print(f"   Files:")
        for f in output_dir.iterdir():
            print(f"     - {f.name}")
        
    except Exception as e:
        print(f"\n‚ùå Export failed: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()
